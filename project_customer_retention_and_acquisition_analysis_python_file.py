# -*- coding: utf-8 -*-
"""Project -Customer Retention and Acquisition Analysis Python File

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ppn3pqxBGKP3hqcbp8iZlADOxp_6SJpY

### üì• Importing Required Libraries and Mounting Google Drive
"""

import pandas as pd
import numpy as np
import matplotlib as mlt
import matplotlib.pyplot as plt
import sklearn as skt
import seaborn as sns
import math
import datetime as dt
import random
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore') # to ignore warnings

from google.colab import drive
drive.mount('/content/drive')

"""### üìä Loading All Dataset"""

df_Repeated_Orders= pd.read_csv(r"/content/drive/MyDrive/PROJECT: CUSTOMER RETENTION AND ACQUISITION/Organic Dec_23 Repeated Orders _ May_24.csv")
df_Repeated_Orders.head(2)
# Tip: Add encoding='utf-8' or 'ISO-8859-1' if you face character errors.

df_All_Channels_Customers = pd.read_csv(r"/content/drive/MyDrive/PROJECT: CUSTOMER RETENTION AND ACQUISITION/All Channels Customers May_24.csv")
df_All_Channels_Customers.head(2)

df_Customer_Acq= pd.read_csv(r"/content/drive/MyDrive/PROJECT: CUSTOMER RETENTION AND ACQUISITION/Organic Dec_23 Customer Acq.csv")
df_Customer_Acq.head(2)

"""### ‚úÖ Validating Datasets for Similarity and **Differences**"""



df_All_Channels_Customers.equals(df_Customer_Acq)

(df_All_Channels_Customers == df_Customer_Acq).sum().sum()           # Total number of matching cells
(df_All_Channels_Customers != df_Customer_Acq).sum().sum()

(df_All_Channels_Customers != df_Customer_Acq).sum().sum()

(df_All_Channels_Customers != df_Customer_Acq).sum()

(df_All_Channels_Customers == df_Customer_Acq)



print(df_Repeated_Orders.shape)
print(df_All_Channels_Customers.shape)
print(df_Customer_Acq.shape)

print(df_Repeated_Orders.columns)
print(df_All_Channels_Customers.columns)
print(df_Customer_Acq.columns)

df_Repeated_Orders.info()

"""### üßπ Data Cleaning and Preprocessing"""

df_Repeated_Orders.isnull().sum()
# sns.heatmap(df_Repeated_Orders.isnull(), cbar=False, yticklabels=False)

df_Repeated_Orders.drop(['ADGROUP_NAME','CREATIVE_NAME','NETWORK'], axis=1, inplace=True)
df_Repeated_Orders.head(1)

df_All_Channels_Customers.info()

df_All_Channels_Customers.drop(['CAMPAIGN_NAME','ADGROUP_NAME','ADGROUP_FORMATED','CREATIVE_NAME','NETWORK'], axis=1, inplace=True)
df_All_Channels_Customers.head(1)

df_Customer_Acq.info()

df_Customer_Acq.drop(['CAMPAIGN_NAME','ADGROUP_NAME','ADGROUP_FORMATED','CREATIVE_NAME','NETWORK'], axis=1, inplace=True)
df_Customer_Acq.head(1)

print(df_Repeated_Orders.shape)
print(df_All_Channels_Customers.shape)
print(df_Customer_Acq.shape)

# For remove duplicates
# Check unique counts per column
unique_counts = df_Repeated_Orders.nunique()
print(unique_counts)
# 708621

df_Repeated_Orders_unique = df_Repeated_Orders.drop_duplicates(subset=['ORDER_ID_RO_IN_WINDOW'])
print(df_Repeated_Orders_unique.shape) # Access shape as an attribute
df_Repeated_Orders_unique.head(1)

df_Repeated_Orders_unique.isnull().sum()

df_Repeated_Orders_unique['VEHICLE_ID'].dtype

df_Repeated_Orders_unique['VEHICLE_TYPE'].dtype

df_Repeated_Orders_unique['VEHICLE_TYPE'].nunique()

df_Repeated_Orders_unique['VEHICLE_TYPE'].unique()

vehicle_mapping = df_Repeated_Orders_unique.dropna(subset=["VEHICLE_TYPE"]).drop_duplicates(subset=["VEHICLE_ID"])[['VEHICLE_ID', 'VEHICLE_TYPE']].set_index('VEHICLE_ID').to_dict()['VEHICLE_TYPE']
print(vehicle_mapping)



df_Repeated_Orders_unique[df_Repeated_Orders_unique['VEHICLE_TYPE'].isna()]

df_Repeated_Orders_unique['VEHICLE_TYPE'].value_counts()

df_Repeated_Orders_unique['VEHICLE_TYPE'] = df_Repeated_Orders_unique['VEHICLE_TYPE'].fillna('2W')

df_Repeated_Orders_unique.isnull().sum()

df_Repeated_Orders_unique[df_Repeated_Orders_unique['GEO_REGION_ID'].isna()]

vehicle_mapping1 = df_Repeated_Orders_unique.dropna(subset=["GEO_REGION_ID"]).drop_duplicates(subset=["GEO_REGION_CITY"])[['GEO_REGION_CITY', 'GEO_REGION_ID']].set_index('GEO_REGION_CITY').to_dict()['GEO_REGION_ID']
print(vehicle_mapping1)

vehicle_mapping1

df_Repeated_Orders_unique['GEO_REGION_ID'] = df_Repeated_Orders_unique['GEO_REGION_CITY'].map(vehicle_mapping1)
df_Repeated_Orders_unique

df_Repeated_Orders_unique.isnull().sum()
# 708620

df_Repeated_Orders_unique.iloc[237054]

CAMPAIGN_NAME = df_Repeated_Orders_unique[df_Repeated_Orders_unique['CAMPAIGN_NAME'].notna()]
CAMPAIGN_NAME



"""Drop CITY_NAME_FROM_CAMPAIGN and CAMPAIGN_NAME"""

df_Repeated_Orders.drop(['CAMPAIGN_NAME','CITY_NAME_FROM_CAMPAIGN'], axis=1, inplace=True)
df_Repeated_Orders.head(1)

df_Repeated_Orders_unique.to_csv('df_Repeated_Orders_unique.csv', index=False)

df_Repeated_Orders_unique_saved=pd.read_csv('df_Repeated_Orders_unique.csv')



"""df_Repeated_Orders_unique_saved **file cleaned**"""



df_All_Channels_Customers.head(2)

unique_counts_acc = df_All_Channels_Customers.nunique()
print(unique_counts_acc)
# 157661

# Identify duplicate rows based on specific columns
duplicates = df_All_Channels_Customers[df_All_Channels_Customers.duplicated(subset=['ADID'])]
duplicates

df_All_Channels_Customers.isnull().sum()
# 157661

df_All_Channels_Customers[df_All_Channels_Customers['GEO_REGION_ID'].isna()]

df_All_Channels_Customers.drop(index=[153860, 154873], inplace=True)

df_All_Channels_Customers.reset_index(drop=True, inplace=True)

df_All_Channels_Customers

df_All_Channels_Customers.isnull().sum()

"""Drop CITY_NAME_FROM_CAMPAIGN"""

df_All_Channels_Customers.drop(['CITY_NAME_FROM_CAMPAIGN'], axis=1, inplace=True)
df_All_Channels_Customers.isnull().sum()



df_All_Channels_Customers.to_csv('df_All_Channels_Customers.csv', index=False)

"""saved"""

df_All_Channels_Customers_saved=pd.read_csv('df_All_Channels_Customers.csv')



df_Customer_Acq.head(2)

unique_counts_ca = df_Customer_Acq.nunique()
print(unique_counts_ca)
# 157661

# Identify duplicate rows based on specific columns
duplicates = df_Customer_Acq[df_Customer_Acq.duplicated(subset=['ADID'])]
duplicates

df_Customer_Acq.isnull().sum()
# 157661

df_Customer_Acq[df_Customer_Acq['GEO_REGION_ID'].isna()]

df_Customer_Acq.drop(index=[153860, 154873], inplace=True)

df_Customer_Acq.reset_index(drop=True, inplace=True)

df_Customer_Acq

df_Customer_Acq.isnull().sum()



"""Drop CITY_NAME_FROM_CAMPAIGN"""

df_Customer_Acq.drop(['CITY_NAME_FROM_CAMPAIGN'], axis=1, inplace=True)
df_Customer_Acq.isnull().sum()

df_Customer_Acq.to_csv('df_Customer_Acq.csv', index=False)

"""Saved"""

df_Customer_Acq=pd.read_csv('df_Customer_Acq.csv')



"""### üßÆ RFM Analysis for Customer Segmentation

**Calculate Key Metrics**

Create a summary DataFrame with the following metrics for each customer:

Recency: Days since the customer's last purchase.

Frequency: Total number of purchases made by the customer.

Monetary Value (Order Value): Total or average amount spent by the customer.
"""

from datetime import datetime, timezone
import pandas as pd

# Current date for recency calculation
current_date = datetime.now(timezone.utc)  # Get current date with UTC timezone

# Convert 'ORDER_DATE' to datetime objects before calculating recency
df_Repeated_Orders_unique_saved['ORDER_DATE'] = pd.to_datetime(df_Repeated_Orders_unique_saved['ORDER_DATE'])

# Ensure 'ORDER_DATE' is timezone-aware (if not already) and convert to UTC
df_Repeated_Orders_unique_saved['ORDER_DATE'] = df_Repeated_Orders_unique_saved['ORDER_DATE'].dt.tz_localize(None).dt.tz_localize('UTC')


# Calculate Recency
recency = df_Repeated_Orders_unique_saved.groupby('CUSTOMER_ID')['ORDER_DATE'].max().apply(lambda x: (current_date - x).days)

# Calculate Frequency
frequency = df_Repeated_Orders_unique_saved.groupby('CUSTOMER_ID')['ORDER_DATE'].count()

# Calculate Monetary Value using 'CUSTOMER_FARE'
monetary = df_Repeated_Orders_unique_saved.groupby('CUSTOMER_ID')['CUSTOMER_FARE'].sum()

# Combine into a single DataFrame
rfm = pd.DataFrame({
    'Recency': recency,
    'Frequency': frequency,
    'MonetaryValue': monetary
})

rfm

# Handle Recency, Frequency, and Monetary Value scoring
rfm['Recency_Score'] = pd.qcut(rfm['Recency'], 4, labels=[4, 3, 2, 1])  # Lower Recency is better

# Use pd.cut for Frequency if duplicate bin edges are an issue
max_frequency = rfm['Frequency'].max()
rfm['Frequency_Score'] = pd.cut(
    rfm['Frequency'],
    bins=[0, 1, 3, 5, max_frequency],  # Define bins
    labels=[1, 2, 3, 4],               # Higher Frequency is better
    include_lowest=True
)

# Use pd.qcut for MonetaryValue as it often has sufficient variation
rfm['Monetary_Score'] = pd.qcut(rfm['MonetaryValue'], 4, labels=[1, 2, 3, 4])  # Higher Monetary is better

# Combine scores into an RFM Score
rfm['RFM_Score'] = rfm['Recency_Score'].astype(int) + rfm['Frequency_Score'].astype(int) + rfm['Monetary_Score'].astype(int)

# Output the result
rfm

def segment_customer(row):
    if row['RFM_Score'] >= 10:
        return 'Champion'
    elif row['RFM_Score'] >= 7:
        return 'Loyal Customer'
    elif row['RFM_Score'] >= 5:
        return 'Potential Loyalist'
    else:
        return 'At Risk'

rfm['Segment'] = rfm.apply(segment_customer, axis=1)
rfm

segment_counts = rfm['Segment'].value_counts()
print(segment_counts)

rfm.reset_index(drop=False, inplace=True)
rfm

rfm.to_csv('rfm.csv', index=False)

# Filter rows where Segment is 'Champion'
champion_ids = rfm[rfm['Segment'] == 'Champion']['CUSTOMER_ID']

# Print all IDs of Champions
champion_ids

import matplotlib.pyplot as plt

segment_counts.plot(kind='bar', color='skyblue', title='Customer Segments')
plt.xlabel('Segment')
plt.ylabel('Number of Customers')
plt.show()

rfm



"""### üîç Behavioral Segmentation"""

# df_Repeated_Orders
df_Repeated_Orders_unique_saved.info()

df_Repeated_Orders_unique_saved.head(10)



"""FOR CUSTOMER ID AND SME/RETAIL"""

# Filter rows where Freq == 4 and calculate the total CUSTOMER_FARE
total_customer_fare = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['FREQ'] == 4]['CUSTOMER_FARE'].sum()

print("Total CUSTOMER_FARE for Freq = 4:", total_customer_fare)

total_customer_fare = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['FREQ'] == 5]['CUSTOMER_FARE'].sum()

print("Total CUSTOMER_FARE for Freq = 5:", total_customer_fare)

total_customer_fare = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['FREQ'] == 6]['CUSTOMER_FARE'].sum()

print("Total CUSTOMER_FARE for Freq = 6:", total_customer_fare)

# Filter rows where Freq is 4 or 5, then calculate the total CUSTOMER_FARE
total_customer_fare = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['FREQ'].isin([5, 6])]['CUSTOMER_FARE'].sum()

print("Total CUSTOMER_FARE for Freq = 6 and Freq = 5:", total_customer_fare)



import pandas as pd
import matplotlib.pyplot as plt

# Filter the DataFrame to include only Freq = 4, 5, and 6
filtered_df = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['FREQ'].isin([4, 5, 6])]

# Group data by Freq and calculate the total CUSTOMER_FARE
total_fare_by_freq = filtered_df.groupby('FREQ')['CUSTOMER_FARE'].sum()

# Plotting the bar graph
plt.bar(total_fare_by_freq.index, total_fare_by_freq.values, color=['blue', 'green', 'orange'], tick_label=['Freq 4', 'Freq 5', 'Freq 6'])

# Adding labels and title
plt.xlabel('Freq')
plt.ylabel('Total CUSTOMER_FARE')
plt.title('Total CUSTOMER_FARE for Freq 4, 5, and 6')

# Display the plot
plt.show()



"""### üßÆ Customer Lifetime Value (CLV) Analysis"""

# Convert dates to datetime format
df_Repeated_Orders_unique_saved['REG_DATE_CUSTOMERS'] = pd.to_datetime(df_Repeated_Orders_unique_saved['REG_DATE_CUSTOMERS'])
df_Repeated_Orders_unique_saved['ORDER_DATE'] = pd.to_datetime(df_Repeated_Orders_unique_saved['ORDER_DATE'])

# Group by CUSTOMER_ID to calculate key metrics
customer_metrics = df_Repeated_Orders_unique_saved.groupby('CUSTOMER_ID').agg(
    total_revenue=('CUSTOMER_FARE', 'sum'),
    order_count=('ORDER_ID_RO_IN_WINDOW', 'count'),
    first_order_date=('ORDER_DATE', 'min'),
    last_order_date=('ORDER_DATE', 'max'),
    reg_date=('REG_DATE_CUSTOMERS', 'min')
).reset_index()
customer_metrics

# Group by CUSTOMER_ID to calculate key metrics
customer_metrics = df_Repeated_Orders_unique_saved.groupby('CUSTOMER_ID').agg(
    total_revenue=('CUSTOMER_FARE', 'sum'),
    order_count=('ORDER_ID_RO_IN_WINDOW', 'count'),
    first_order_date=('ORDER_DATE', 'min'),
    last_order_date=('ORDER_DATE', 'max'),
    reg_date=('REG_DATE_CUSTOMERS', 'min')
).reset_index()
customer_metrics

# Calculate customer lifespan in months
customer_metrics['customer_lifespan_months'] = (
    (customer_metrics['last_order_date'] - customer_metrics['first_order_date']).dt.days / 30
).round()

# Avoid division by zero for avg_monthly_revenue
customer_metrics['avg_monthly_revenue'] = (
    customer_metrics['total_revenue'] / customer_metrics['customer_lifespan_months']
).where(customer_metrics['customer_lifespan_months'] > 0, 0)

# Calculate Historical CLV (set to 0 if lifespan is 0)
customer_metrics['Historical_clv/Predictive_clv'] = customer_metrics['avg_monthly_revenue'] * customer_metrics['customer_lifespan_months']

customer_metrics['clv_segment'] = pd.qcut(
    customer_metrics['Historical_clv/Predictive_clv'],
    q=3,  # Reduce number of bins
    labels=['Low', 'Medium', 'High'],
    duplicates='drop'
)
bins = [0, 1, 1000, 2000, 50000, 150000, float('inf')]  # Define bin edges
labels = ['Low', 'Low_Medium', 'Medium', 'Medium_High', 'High', 'Extreme_High']  # Match labels

customer_metrics['clv_segment'] = pd.cut(
    customer_metrics['Historical_clv/Predictive_clv'],
    bins=bins,
    labels=labels,
    include_lowest=True
)

# Instead, directly fill NaN values using fillna
customer_metrics['clv_segment'] = customer_metrics['clv_segment'].fillna('Low')

# Format date columns to 'YYYY-MM-DD'
customer_metrics['first_order_date'] = customer_metrics['first_order_date'].dt.strftime('%Y-%m-%d')
customer_metrics['last_order_date'] = customer_metrics['last_order_date'].dt.strftime('%Y-%m-%d')
customer_metrics['reg_date'] = customer_metrics['reg_date'].dt.strftime('%Y-%m-%d')


# Verify no NaN or inf values
# print(customer_metrics[['avg_monthly_revenue', 'historical_clv', 'clv_segment']].isna().sum())
# print(customer_metrics[['avg_monthly_revenue', 'historical_clv']].replace([float('inf'), -float('inf')], 0).describe())
print(customer_metrics['clv_segment'].unique())
print(customer_metrics['clv_segment'].value_counts())

# Display the resulting DataFrame
customer_metrics

# Make sure 'clv_segment' is treated as a category (optional but good practice)
import plotly.express as px
customer_metrics['clv_segment'] = customer_metrics['clv_segment'].astype('category')

# Create the pie chart
fig_pie = px.pie(
    customer_metrics,
    names='clv_segment',
    title='CLV Segment Distribution',
    hole=0.4,
    color_discrete_sequence=px.colors.qualitative.Set2
)

# Update trace for better readability
fig_pie.update_traces(textinfo='percent+label')

# Show the plot
fig_pie.show()

customer_metrics.to_csv('CLV-customer_metrics.csv', index=False)



Medium_clv= customer_metrics[customer_metrics['clv_segment'] == 'Medium']
Medium_clv

High_Predictive_CLV = customer_metrics[customer_metrics['clv_segment'] == 'High']
High_Predictive_CLV



df_Repeated_Orders_unique_saved.columns

customer_metrics



# Customers with high RFM & high CLV ‚Üí ‚ÄúLoyal Premiums

"""CLV"""



df_Repeated_Orders_unique_saved

total_customer_fare_for_clv = df_Repeated_Orders_unique_saved['CUSTOMER_FARE'].sum()

print("Total Salary:", total_customer_fare_for_clv)

119016983.46999998+117118550.04



total_customer_fare_for_sme = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['FREQ_SME_RETAILS'] == 'SME']['CUSTOMER_FARE'].sum()

print("Total CUSTOMER_FARE for sme:", total_customer_fare_for_sme)

total_customer_fare_for_Retail = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['FREQ_SME_RETAILS'] == 'Retail']['CUSTOMER_FARE'].sum()

print("Total CUSTOMER_FARE for retail:", total_customer_fare_for_Retail)

df_Customer_Acq.head(1)

total_customer_acq_fare_for_clv = df_Customer_Acq['CUSTOMER_FARE'].sum()

print("Total Salary:", total_customer_acq_fare_for_clv)

total_Customer_Acq_fare_for_sme = df_Customer_Acq[df_Customer_Acq['FREQ_SME_RETAILS'] == 'SME']['CUSTOMER_FARE'].sum()

print("Total CUSTOMER_FARE for sme:", total_Customer_Acq_fare_for_sme)

total_Customer_Acq_fare_for_Retail = df_Customer_Acq[df_Customer_Acq['FREQ_SME_RETAILS'] == 'Retail']['CUSTOMER_FARE'].sum()

print("Total CUSTOMER_FARE for Retail:", total_Customer_Acq_fare_for_Retail)



total_customer_fare_for_sme/total_Customer_Acq_fare_for_sme

# Sort by OrderDate and keep the first transaction for each CustomerID
first_order_table = df_All_Channels_Customers_saved.sort_values('ORDER_DATE').drop_duplicates('CUSTOMER_ID', keep='first')

first_order_table

first_order_table.to_csv('first_order_table.csv', index=False)

first_order_table_fare_for_clv = first_order_table['CUSTOMER_FARE'].sum()

print("Total Salary:", first_order_table_fare_for_clv)

total_first_order_table_fare_for_sme = first_order_table[first_order_table['FREQ_SME_RETAILS'] == 'SME']['CUSTOMER_FARE'].sum()

print("Total first_order_table CUSTOMER_FARE for sme:", total_first_order_table_fare_for_sme)

total_first_order_table_fare_for_Retail = first_order_table[first_order_table['FREQ_SME_RETAILS'] == 'Retail']['CUSTOMER_FARE'].sum()

print("Total first_order_table CUSTOMER_FARE for Retail:", total_first_order_table_fare_for_Retail)

clv_for_sme=total_customer_fare_for_sme/total_first_order_table_fare_for_sme
clv_for_sme

clv_for_retail=total_customer_fare_for_Retail/total_first_order_table_fare_for_Retail
clv_for_retail



# Step 1: Filter SME customers
sme_customers = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['FREQ_SME_RETAILS'] == 'SME']
sme_customers

# Count total CUSTOMER_IDs
total_customers = sme_customers['CUSTOMER_ID'].count()

# Count unique CUSTOMER_IDs
unique_customers = sme_customers['CUSTOMER_ID'].nunique()

# Count non-unique CUSTOMER_IDs (duplicates)
non_unique_customers = total_customers - unique_customers

print(f"Total CUSTOMER_IDs: {total_customers}")
print(f"Unique CUSTOMER_IDs: {unique_customers}")
print(f"Non-unique CUSTOMER_IDs: {non_unique_customers}")

# Step 2: Calculate Total Revenue for each SME customer
total_revenue_of_sme_customers = sme_customers.groupby('CUSTOMER_ID')['CUSTOMER_FARE'].sum()

# Step 3: Calculate Average Order Value (AOV)
total_orders_sme = sme_customers.groupby('CUSTOMER_ID')['ORDER_ID_RO_IN_WINDOW'].count()

aov = total_revenue_of_sme_customers / total_orders_sme
aov.mean()

# Step 4: Calculate Purchase Frequency (PF)
purchase_frequency = total_orders_sme.mean()
purchase_frequency

# Step 5: Calculate Customer Lifetime (CL)
sme_customers['ORDER_DATE'] = pd.to_datetime(sme_customers['ORDER_DATE'])
customer_lifetime = sme_customers.groupby('CUSTOMER_ID').apply(
    lambda x: (x['ORDER_DATE'].max() - x['ORDER_DATE'].min()).days / 365
).mean()
customer_lifetime

# Convert ORDER_DATE to datetime if not already done
sme_customers['ORDER_DATE'] = pd.to_datetime(sme_customers['ORDER_DATE'])

# Initialize a list to store the customer lifetimes
customer_lifetimes = []

# Loop through each unique CUSTOMER_ID
for customer_id in sme_customers['CUSTOMER_ID'].unique():
    # Filter data for the current customer
    customer_data = sme_customers[sme_customers['CUSTOMER_ID'] == customer_id]

    # Find the first and last order dates
    first_order_date = customer_data['ORDER_DATE'].min()
    last_order_date = customer_data['ORDER_DATE'].max()

    # Calculate the customer lifetime in years
    lifetime_years = (last_order_date - first_order_date).days / 365

    # Append the lifetime to the list
    customer_lifetimes.append(lifetime_years)

# Calculate the average customer lifetime
average_customer_lifetime = sum(customer_lifetimes) / len(customer_lifetimes)

print(f"Average Customer Lifetime (in years): {average_customer_lifetime}")

# Step 6: Calculate CLV
clv = aov.mean() * purchase_frequency * customer_lifetime

print("Customer Lifetime Value (CLV) for SME customers:", clv)





import pandas as pd

# Step 1: Filter Retail customers

Retail_customers = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['FREQ_SME_RETAILS'] == 'Retail']


# Step 2: Calculate Total Revenue for each Retail customer
total_revenue_of_Retail_customers = Retail_customers.groupby('CUSTOMER_ID')['CUSTOMER_FARE'].sum()

# Step 3: Calculate Average Order Value (AOV)
total_orders_Retail = Retail_customers.groupby('CUSTOMER_ID')['ORDER_ID_RO_IN_WINDOW'].count()
aov = total_revenue_of_Retail_customers / total_orders_Retail

# Step 4: Calculate Purchase Frequency (PF)
purchase_frequency = total_orders_Retail.mean()

# Step 5: Calculate Customer Lifetime (CL)
Retail_customers['ORDER_DATE'] = pd.to_datetime(Retail_customers['ORDER_DATE'])
customer_lifetime = Retail_customers.groupby('CUSTOMER_ID').apply(
    lambda x: (x['ORDER_DATE'].max() - x['ORDER_DATE'].min()).days / 365
).mean()

# Step 6: Calculate CLV
clv = aov.mean() * purchase_frequency * customer_lifetime

print("Customer Lifetime Value (CLV) for Retail customers:", clv)



import pandas as pd

# Step 1: Filter 2W customers
W_customers = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['VEHICLE_TYPE'] == '2W']


# Step 2: Calculate Total Revenue for each 2W customer
total_revenue = W_customers.groupby('CUSTOMER_ID')['CUSTOMER_FARE'].sum()

# Step 3: Calculate Average Order Value (AOV)
total_orders = W_customers.groupby('CUSTOMER_ID')['ORDER_ID_RO_IN_WINDOW'].count()
aov = total_revenue / total_orders

# Step 4: Calculate Purchase Frequency (PF)
purchase_frequency = total_orders.mean()

# Step 5: Calculate Customer Lifetime (CL)
W_customers['ORDER_DATE'] = pd.to_datetime(W_customers['ORDER_DATE'])
customer_lifetime = W_customers.groupby('CUSTOMER_ID').apply(
    lambda x: (x['ORDER_DATE'].max() - x['ORDER_DATE'].min()).days / 365
).mean()

# Step 6: Calculate CLV
clv = aov.mean() * purchase_frequency * customer_lifetime

print("Customer Lifetime Value (CLV) for 2W customers:", clv)

HCV_and_LCV_customers = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['VEHICLE_TYPE'].isin(['HCV', 'LCV'])]
HCV_and_LCV_customers.head()

import pandas as pd

# Step 1: Filter SME customers
HCV_and_LCV_customers = df_Repeated_Orders_unique_saved[df_Repeated_Orders_unique_saved['VEHICLE_TYPE'].isin(['HCV', 'LCV'])]

# Step 2: Calculate Total Revenue for each SME customer
total_revenue_HCV_and_LCV_customers = HCV_and_LCV_customers.groupby('CUSTOMER_ID')['CUSTOMER_FARE'].sum()

# Step 3: Calculate Average Order Value (AOV)
total_orders_HCV_and_LCV_customers = HCV_and_LCV_customers.groupby('CUSTOMER_ID')['ORDER_ID_RO_IN_WINDOW'].count()
aov = total_revenue_HCV_and_LCV_customers / total_orders_HCV_and_LCV_customers

# Step 4: Calculate Purchase Frequency (PF)
purchase_frequency = total_orders_HCV_and_LCV_customers.mean()

# Step 5: Calculate Customer Lifetime (CL)
HCV_and_LCV_customers['ORDER_DATE'] = pd.to_datetime(HCV_and_LCV_customers['ORDER_DATE'])
customer_lifetime = HCV_and_LCV_customers.groupby('CUSTOMER_ID').apply(
    lambda x: (x['ORDER_DATE'].max() - x['ORDER_DATE'].min()).days / 365
).mean()

# Step 6: Calculate CLV
clv = aov.mean() * purchase_frequency * customer_lifetime

print("Customer Lifetime Value (CLV) for HCV AND LCV customers:", clv)

"""### üìà Customer Lifetime Value (CLV) Analysis Dashboard"""

!pip install statsmodels

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

# Load customer_metrics file
df = pd.read_csv("CLV-customer_metrics.csv")

# Convert date columns back to datetime (if needed)
date_cols = ["first_order_date", "last_order_date", "reg_date"]
for col in date_cols:
    df[col] = pd.to_datetime(df[col], errors='coerce')

# Clean column names
df.columns = df.columns.str.strip()

# 1. CLV Segment Distribution (Pie Chart)
fig_pie = px.pie(
    df,
    names='clv_segment',
    title='CLV Segment Distribution',
    hole=0.4,
    color_discrete_sequence=px.colors.qualitative.Set2
)
fig_pie.update_traces(textinfo='percent+label')

# 2. CLV vs Customer Lifespan (Scatter)
fig_scatter_lifespan = px.scatter(
    df,
    x='customer_lifespan_months',
    y='Historical_clv/Predictive_clv',
    color='clv_segment',
    title='CLV vs Customer Lifespan',
    labels={
        'customer_lifespan_months': 'Lifespan (Months)',
        'Historical_clv/Predictive_clv': 'CLV'
    },
    size=df['total_revenue'].apply(lambda x: x if x > 0 else 1),  # ‚úÖ fixed size
    color_discrete_sequence=px.colors.qualitative.Set1
)


# 3. CLV vs Order Count (Scatter with Trend)
fig_scatter_orders = px.scatter(
    df,
    x='order_count',
    y='Historical_clv/Predictive_clv',
    color='clv_segment',
    trendline='ols',
    title='CLV vs Order Count',
    labels={
        'order_count': 'Order Count',
        'Historical_clv/Predictive_clv': 'CLV'
    },
    color_discrete_sequence=px.colors.qualitative.Dark2
)

# 3. Bar Chart: Avg Revenue per CLV Segment
segment_avg = df.groupby('clv_segment')['total_revenue'].mean().reset_index()
fig_bar = px.bar(
    segment_avg,
    x='clv_segment',
    y='total_revenue',
    title='Average Revenue by CLV Segment',
    color='clv_segment',
    color_discrete_sequence=px.colors.sequential.Teal
)

# 4. Customer Lifespan Distribution (Box Plot)
fig_lifespan = px.box(
    df,
    y='customer_lifespan_months',
    title='Customer Lifespan Distribution (Months)',
    points='outliers',
    color_discrete_sequence=['#636EFA']
)

# Save all to one HTML file
with open("clv_dashboard.html", "w") as f:
    f.write(fig_pie.to_html(full_html=False, include_plotlyjs='cdn'))
    f.write(fig_scatter_lifespan.to_html(full_html=False, include_plotlyjs=False))
    f.write(fig_scatter_orders.to_html(full_html=False, include_plotlyjs=False))
    f.write(fig_bar.to_html(full_html=False, include_plotlyjs=False))
    f.write(fig_lifespan.to_html(full_html=False, include_plotlyjs=False))

print("‚úÖ Dashboard saved as: clv_dashboard.html")

"""### üí•Promotional Effectiveness"""

customer_metrics.info()

df_Repeated_Orders_unique_saved.info()



# from sklearn.cluster import KMeans
# # Example: K-means clustering
# X = customer_metrics[['total_revenue', 'order_count', 'customer_lifespan_months']]
# kmeans = KMeans(n_clusters=5, random_state=42)
# customer_metrics['customer_segment'] = kmeans.fit_predict(X)

# Example: Filter campaigns by customer segments
import pandas as pd

# Merge campaign data with customer data based on CUSTOMER_ID
df_Repeated_Orders_unique_saved = pd.merge(df_Repeated_Orders_unique_saved, customer_metrics, on='CUSTOMER_ID', how='inner')

# Group by campaign name and customer segment to calculate total revenue
campaign_summary = df_Repeated_Orders_unique_saved.groupby(['CAMPAIGN_NAME',])['total_revenue'].sum().reset_index()

# Evaluate campaign performance by segment
campaign_summary

df_Repeated_Orders_unique_saved

df_Repeated_Orders_unique_saved.info()

df_Repeated_Orders_unique_saved.to_csv('df_Repeated_Orders_unique_saved.csv', index=False)

# Example: Analyze promotion effectiveness
promotion_effectiveness = df_Repeated_Orders_unique_saved.groupby(['CAMPAIGN_NAME']).agg({
    'total_revenue': 'sum',
    'order_count': 'sum',
    'CONVERSION_WINDOW': 'mean'
}).reset_index()

# Sort promotions by effectiveness
promotion_effectiveness = promotion_effectiveness.sort_values(by='total_revenue', ascending=False)
promotion_effectiveness.head(11)

promotion_effectiveness.to_csv('promotion_effectiveness.csv', index=False)

"""###  üìà Pattern Analysis and Forecasting"""

df_Repeated_Orders_unique_saved.columns

# Convert ORDER_DATE to datetime if it is not already
df_Repeated_Orders_unique_saved['ORDER_DATE'] = pd.to_datetime(df_Repeated_Orders_unique_saved['ORDER_DATE'])

# Aggregate data by month or any other time period (e.g., weekly, daily)
df_monthly_orders = df_Repeated_Orders_unique_saved.groupby(df_Repeated_Orders_unique_saved['ORDER_DATE'].dt.to_period('M')).agg({
    'ORDER_ID_RO_IN_WINDOW': 'count',
    'total_revenue': 'sum'
}).reset_index()

# Plotting the trends
plt.figure(figsize=(10, 6))
plt.plot(df_monthly_orders['ORDER_DATE'].astype(str), df_monthly_orders['total_revenue'], label='Total Revenue')
plt.xlabel('Month')
plt.ylabel('Total Revenue')
plt.title('Monthly Trend in Orders and Revenue')
plt.xticks(rotation=45)
plt.legend()
plt.show()

df_monthly_orders

df_monthly_orders_dataframe = pd.DataFrame(df_monthly_orders)
print(df_monthly_orders_dataframe)

df_monthly_orders_dataframe= df_monthly_orders_dataframe.rename(columns={'ORDER_DATE': 'Months'})

df_monthly_orders_dataframe = df_monthly_orders_dataframe[['Months', 'total_revenue']]
df_monthly_orders_dataframe

df_Repeated_Orders_trend = df_Repeated_Orders_unique_saved.groupby(df_Repeated_Orders_unique_saved['ORDER_DATE']).agg({
    'total_revenue': 'sum'
}).reset_index()

df_Repeated_Orders_trend

df_Repeated_Orders_unique_saved.info()

df_Repeated_Orders_trend['Date'] = df_Repeated_Orders_trend['ORDER_DATE'].dt.strftime('%Y-%m-%d')
df_Repeated_Orders_trend = df_Repeated_Orders_trend[['Date', 'total_revenue']]
df_Repeated_Orders_trend
# 708620

df_Repeated_Orders_trend = df_Repeated_Orders_trend.groupby('Date')['total_revenue'].sum()
df_Repeated_Orders_trend

df_Repeated_Orders_trend=df_Repeated_Orders_trend.reset_index()
df_Repeated_Orders_trend



df_Repeated_Orders_trend.head(11)

plt.figure(figsize=(40, 6))
plt.plot(df_Repeated_Orders_trend['Date'], df_Repeated_Orders_trend['total_revenue'])
plt.xlabel('Date')
plt.ylabel('Total Revenue')
plt.title('Total Revenue Over Time')
# Rotate x-axis labels
plt.xticks(rotation=90)
plt.show()

df_Repeated_Orders_trend

df = df_Repeated_Orders_trend.reset_index()
df

import pandas as pd

# Assuming your DataFrame is named 'df'
# Convert 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Resample to weekly frequency (W-SUN = weeks ending on Sunday)
weekly_df = df.resample('W-SUN', on='Date')['total_revenue'].sum().reset_index()

# Rename columns for clarity
weekly_df.columns = ['Week_Ending', 'Total_Revenue']

# Convert 'Total_Revenue' to integer type
weekly_df['Total_Revenue'] = weekly_df['Total_Revenue'].astype(int)

# Ensure 'Week_Ending' remains in datetime format
weekly_df['Week_Ending'] = pd.to_datetime(weekly_df['Week_Ending'])

# Show results
print(weekly_df)

weekly_df=weekly_df.iloc[1:-1]

weekly_df

plt.figure(figsize=(20, 6))
plt.plot(weekly_df['Week_Ending'], weekly_df['Total_Revenue'])
plt.xlabel('weekly')
plt.ylabel('Total Revenue')
plt.title('Total Revenue Over Time')
# Rotate x-axis labels
plt.xticks(rotation=90)
plt.show()

df=weekly_df
df

# Plotting the weekly revenue trend
plt.figure(figsize=(10, 6))
plt.plot(df['Week_Ending'].astype(str), df['Total_Revenue'], marker='o', linestyle='-', color='g', label='Total Revenue')
plt.xlabel('Week')
plt.ylabel('Total Revenue')
plt.title('Weekly Revenue Trend')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()



df['Week_Ending'] = pd.to_datetime(df['Week_Ending'])

df['Week_Ending'].dtype

! pip install statsmodels

!pip install statsmodels
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Assuming 'df' is the DataFrame with your data
# Split the data into training and test sets (last 6 weeks as the test set)
split_point = len(df) - 6

# Split the data
train = df['Total_Revenue'][:split_point]  # First 80% for training
test = df['Total_Revenue'][split_point:]   # Remaining 20% for testing
# Print the results
print("Training data:")
print(train)
print("\nTesting data:")
print(test)

# Set a smaller seasonal_periods value (e.g., 4 for quarterly data, or a value less than len(train) // 2)
seasonal_periods = 4  # Example: Quarterly seasonality

# Check if seasonal_periods is valid based on the length of the training data
if seasonal_periods > len(train) // 2:
    seasonal_periods = len(train) // 2  # Limit seasonal_periods to half the training data length
    print(f"Seasonal periods adjusted to {seasonal_periods} due to insufficient training data.")

# Fit the Additive Holt-Winters model
additive_model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=seasonal_periods).fit()
additive_forecast = additive_model.forecast(len(test))

# Fit the Multiplicative Holt-Winters model
multiplicative_model = ExponentialSmoothing(train, trend='add', seasonal='mul', seasonal_periods=seasonal_periods).fit()
multiplicative_forecast = multiplicative_model.forecast(len(test))

# Calculate RMSE and MAE for the Additive model
additive_rmse = np.sqrt(mean_squared_error(test, additive_forecast))
additive_mae = mean_absolute_error(test, additive_forecast)

# Calculate RMSE and MAE for the Multiplicative model
multiplicative_rmse = np.sqrt(mean_squared_error(test, multiplicative_forecast))
multiplicative_mae = mean_absolute_error(test, multiplicative_forecast)

# Print the results
print(f"Additive Model RMSE: {additive_rmse:.4f}")
print(f"Additive Model MAE: {additive_mae:.4f}")
print(f"Multiplicative Model RMSE: {multiplicative_rmse:.4f}")
print(f"Multiplicative Model MAE: {multiplicative_mae:.4f}")

# Automatically select the better model based on RMSE and MAE
if additive_rmse < multiplicative_rmse and additive_mae < multiplicative_mae:
    print("Using Additive model for forecasting")
    selected_trend = 'add'
    selected_seasonal = 'add'
else:
    print("Using Multiplicative model for forecasting")
    selected_trend = 'add'
    selected_seasonal = 'mul'

# Convert the 'Week_Ending' column to datetime with the correct format if it's not already datetime
# df['Week_Ending'] = pd.to_datetime(df['Week_Ending']) # No need to convert if it's already datetime

# Reset the index to bring 'Week_Ending' back as a column
df = df.reset_index()

# Convert to datetime if not already
df['Week_Ending'] = pd.to_datetime(df['Week_Ending'])

# Set 'Week_Ending' as the index
df.set_index('Week_Ending', inplace=True)

# Step 1: Fit the selected Holt-Winters model
model = ExponentialSmoothing(
    df['Total_Revenue'],
    trend=selected_trend,           # Selected trend
    seasonal=selected_seasonal,     # Selected seasonality
    seasonal_periods=seasonal_periods            # 12 months in a year
).fit()

# Step 2: Forecast the next 12 weeks (adjust if needed)
forecast = model.forecast(steps=12)

# Step 3: Create a new DataFrame to store results
forecast_index = pd.date_range(start=df.index[-1] + pd.DateOffset(weeks=1), periods=12, freq='W-SUN')
forecast_df = pd.DataFrame({'Total_Revenue': forecast}, index=forecast_index)

# Step 4: Plot original data and forecast
plt.figure(figsize=(12, 6))
plt.plot(df.index, df['Total_Revenue'], label='Historical Data', marker='o')
plt.plot(forecast_df.index, forecast_df['Total_Revenue'], label='Forecast', marker='o', color='orange')
plt.title('Holt-Winters Forecast')
plt.xlabel('Week')  # Changed to 'Week' for weekly data
plt.ylabel('Total Revenue')
plt.legend()
plt.grid()
plt.show()

# Step 5: Display forecasted values
print("Forecasted Values:")
print(forecast_df)

# Step 6: Save historical data and forecast to CSV
# Concatenate historical data and forecast data
combined_df = pd.concat([df, forecast_df])

# Save to CSV
combined_df.to_csv('total_revenue_forecast.csv')
print("Data saved to 'total_revenue_forecast.csv'")





from scipy.stats import zscore

# Compute Z-scores for CUSTOMER_FARE (order value equivalent)
df_Repeated_Orders['Fare_Z_Score'] = zscore(df_Repeated_Orders['CUSTOMER_FARE'])

# Define threshold for anomaly detection (values above 3 are considered outliers)
threshold = 3

# Identify anomalies
anomalies = df_Repeated_Orders[df_Repeated_Orders['Fare_Z_Score'].abs() > threshold]

# Display anomalies
print("Anomalous Orders:")
print(anomalies[['CUSTOMER_ID', 'CUSTOMER_FARE', 'Fare_Z_Score']])







"""### üî• Cohort Analysis and Visualization"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset (Assuming df_Repeated_Orders_unique_saved contains 'CUSTOMER_ID' and 'ORDER_DATE')
df = df_Repeated_Orders_unique_saved.copy()

# Convert ORDER_DATE to datetime format
df['ORDER_DATE'] = pd.to_datetime(df['ORDER_DATE'])

# Extract Cohort Month (first purchase month of each customer)
df['Cohort_Month'] = df.groupby('CUSTOMER_ID')['ORDER_DATE'].transform('min').dt.to_period('M')

# Extract Order Year and Month
df['Order_Year'] = df['ORDER_DATE'].dt.year
df['Order_Month'] = df['ORDER_DATE'].dt.month

# Create Cohort Index (time difference from first purchase)
df['Cohort_Index'] = (df['Order_Year'] - df['Cohort_Month'].dt.year) * 12 + (df['Order_Month'] - df['Cohort_Month'].dt.month) + 1

# Create Cohort Table (Pivot)
cohort_counts = df.groupby(['Cohort_Month', 'Cohort_Index'])['CUSTOMER_ID'].nunique().unstack()

# Calculate Retention Rates
cohort_sizes = cohort_counts.iloc[:, 0]  # First column (month 1) is the cohort size
retention_matrix = cohort_counts.divide(cohort_sizes, axis=0)

# Calculate Churn Rates (1 - Retention Rate)
churn_matrix = 1 - retention_matrix

# 1Ô∏è‚É£ Heatmap for Retention Trends
plt.figure(figsize=(12, 6))
sns.heatmap(retention_matrix, annot=True, fmt=".0%", cmap="Blues")
plt.title("Customer Retention by Cohort")
plt.ylabel("Cohort Month")
plt.xlabel("Cohort Index (Months)")
plt.show()

# 2Ô∏è‚É£ Line Plot for Retention Rate Trends
plt.figure(figsize=(10, 5))
for index in retention_matrix.index:
    plt.plot(retention_matrix.columns, retention_matrix.loc[index], marker="o", label=str(index))

plt.title("Customer Retention Over Time")
plt.xlabel("Cohort Index (Months)")
plt.ylabel("Retention Rate")
plt.legend(title="Cohort Month", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset (Assuming df_Repeated_Orders_unique_saved contains 'CUSTOMER_ID' and 'ORDER_DATE')
df = df_Repeated_Orders_unique_saved.copy()

# Convert ORDER_DATE to datetime format
df['ORDER_DATE'] = pd.to_datetime(df['ORDER_DATE'])

# Extract Cohort Month (first purchase month of each customer)
df['Cohort_Month'] = df.groupby('CUSTOMER_ID')['ORDER_DATE'].transform('min').dt.to_period('M')

# Extract Order Year and Month
df['Order_Year'] = df['ORDER_DATE'].dt.year
df['Order_Month'] = df['ORDER_DATE'].dt.month

# Create Cohort Index (time difference from first purchase)
df['Cohort_Index'] = (df['Order_Year'] - df['Cohort_Month'].dt.year) * 12 + (df['Order_Month'] - df['Cohort_Month'].dt.month) + 1

# Create Cohort Table (Pivot)
cohort_counts = df.groupby(['Cohort_Month', 'Cohort_Index'])['CUSTOMER_ID'].nunique().unstack()

# Calculate Retention Rates
cohort_sizes = cohort_counts.iloc[:, 0]  # First column (month 1) is the cohort size
retention_matrix = cohort_counts.divide(cohort_sizes, axis=0)

# Calculate Churn Rates (1 - Retention Rate)
churn_matrix = 1 - retention_matrix

# 1Ô∏è‚É£ Heatmap for Retention Trends
plt.figure(figsize=(12, 6))
sns.heatmap(retention_matrix, annot=True, fmt=".0%", cmap="Blues")
plt.title("Customer Retention by Cohort")
plt.ylabel("Cohort Month")
plt.xlabel("Cohort Index (Months)")
plt.show()

# 2Ô∏è‚É£ Line Plot for Retention Rate Trends
plt.figure(figsize=(10, 5))
for index in retention_matrix.index:
    plt.plot(retention_matrix.columns, retention_matrix.loc[index], marker="o", label=str(index))

plt.title("Customer Retention Over Time")
plt.xlabel("Cohort Index (Months)")
plt.ylabel("Retention Rate")
plt.legend(title="Cohort Month", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# 3Ô∏è‚É£ Heatmap for Churn Rates
plt.figure(figsize=(12, 6))
sns.heatmap(churn_matrix, annot=True, fmt=".0%", cmap="Reds") #added cmap
plt.title("Customer Churn by Cohort")
plt.ylabel("Cohort Month")
plt.xlabel("Cohort Index (Months)")
plt.show()





import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np  # Ensure NumPy is imported

# Select only numerical features
numerical_df = df.select_dtypes(include=np.number)

# Drop the unwanted column (replace 'Column_Name' with the actual column name)
numerical_df = numerical_df.drop(columns=['Historical_clv/Predictive_clv'])

# Calculate the correlation matrix
correlation_matrix = numerical_df.corr()

# Plot the heatmap
plt.figure(figsize=(20, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap After Column Removal')
plt.show()



df

import pandas as pd

# Set option to display all columns
pd.set_option('display.max_columns', None)

# Optionally, you can also adjust the number of rows if needed
# pd.set_option('display.max_rows', None)

# Now when you display your DataFrame, it will show all columns


# Display top 5 rows with all columns
df.head()



from scipy.stats import ttest_ind


# Filter data for two campaigns (replace 'CampaignA' and 'CampaignB' with actual campaign names)
campaign_a = df_Repeated_Orders_unique[df_Repeated_Orders_unique['CAMPAIGN_NAME'] == 'porter apk download']['CUSTOMER_FARE']
campaign_b = df_Repeated_Orders_unique[df_Repeated_Orders_unique['CAMPAIGN_NAME'] == 'app for parcel delivery']['CUSTOMER_FARE']

# Perform t-test
t_stat, p_value = ttest_ind(campaign_a, campaign_b)

# Output results
print(f"T-statistic: {t_stat:.4f}, P-value: {p_value:.4f}")

# Interpret results
if p_value < 0.05:
    print("There is a statistically significant difference between the two campaigns.")
else:
    print("There is no statistically significant difference between the two campaigns.")



"""### üìÖ RFM Analysis Dashboard"""

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Load the processed RFM file
rfm = pd.read_csv("rfm.csv")

# 1. Customer Segment Distribution
fig_segment = px.pie(
    rfm,
    names="Segment",
    title="Customer Segment Distribution",
    hole=0.4,
    color_discrete_sequence=px.colors.qualitative.Set2
)
fig_segment.update_traces(textposition='inside', textinfo='percent+label')

# 2. RFM Score Histogram
fig_rfm_score = px.histogram(
    rfm,
    x="RFM_Score",
    nbins=10,
    title="Distribution of RFM Scores",
    color="Segment",
    color_discrete_sequence=px.colors.qualitative.Set3
)

# 3. Recency, Frequency, and Monetary Distributions
fig_rfm_dists = make_subplots(rows=1, cols=3, subplot_titles=("Recency", "Frequency", "Monetary Value"))

fig_rfm_dists.add_trace(go.Box(y=rfm["Recency"], name="Recency", marker_color="red"), row=1, col=1)
fig_rfm_dists.add_trace(go.Box(y=rfm["Frequency"], name="Frequency", marker_color="green"), row=1, col=2)
fig_rfm_dists.add_trace(go.Box(y=rfm["MonetaryValue"], name="Monetary", marker_color="blue"), row=1, col=3)
fig_rfm_dists.update_layout(title_text="RFM Metric Distributions", showlegend=False)

# 4. Segment vs Monetary Value (Bar)
segment_avg_monetary = rfm.groupby("Segment")["MonetaryValue"].mean().sort_values(ascending=False).reset_index()
fig_monetary_bar = px.bar(
    segment_avg_monetary,
    x="Segment",
    y="MonetaryValue",
    title="Average Monetary Value per Segment",
    color="Segment",
    color_discrete_sequence=px.colors.qualitative.Pastel
)

# Show all plots
fig_segment.show()
fig_rfm_score.show()
fig_rfm_dists.show()
fig_monetary_bar.show()



import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from plotly.offline import plot

# Load your CSV file
rfm = pd.read_csv("rfm.csv")

# 1. Customer Segment Distribution
fig_segment = px.pie(
    rfm,
    names="Segment",
    title="Customer Segment Distribution",
    hole=0.4,
    color_discrete_sequence=px.colors.qualitative.Set2
)
fig_segment.update_traces(textposition='inside', textinfo='percent+label')

# 2. RFM Score Histogram
fig_rfm_score = px.histogram(
    rfm,
    x="RFM_Score",
    nbins=10,
    title="Distribution of RFM Scores",
    color="Segment",
    color_discrete_sequence=px.colors.qualitative.Set3
)

# 3. Recency, Frequency, and Monetary Distributions
fig_rfm_dists = make_subplots(rows=1, cols=3, subplot_titles=("Recency", "Frequency", "Monetary Value"))
fig_rfm_dists.add_trace(go.Box(y=rfm["Recency"], name="Recency", marker_color="red"), row=1, col=1)
fig_rfm_dists.add_trace(go.Box(y=rfm["Frequency"], name="Frequency", marker_color="green"), row=1, col=2)
fig_rfm_dists.add_trace(go.Box(y=rfm["MonetaryValue"], name="Monetary", marker_color="blue"), row=1, col=3)
fig_rfm_dists.update_layout(title_text="RFM Metric Distributions", showlegend=False)

# 4. Segment vs Monetary Value (Bar)
segment_avg_monetary = rfm.groupby("Segment")["MonetaryValue"].mean().sort_values(ascending=False).reset_index()
fig_monetary_bar = px.bar(
    segment_avg_monetary,
    x="Segment",
    y="MonetaryValue",
    title="Average Monetary Value per Segment",
    color="Segment",
    color_discrete_sequence=px.colors.qualitative.Pastel
)

# Combine into one HTML file
html_all = "<html><head><title>RFM Dashboard</title><script src='https://cdn.plot.ly/plotly-latest.min.js'></script></head><body>"
html_all += "<h1 style='text-align:center'>RFM Analysis Dashboard</h1>"

for fig in [fig_segment, fig_rfm_score, fig_rfm_dists, fig_monetary_bar]:
    inner_html = plot(fig, include_plotlyjs=False, output_type='div')
    html_all += inner_html + "<hr>"

html_all += "</body></html>"

# Save it
with open("rfm_dashboard.html", "w", encoding="utf-8") as f:
    f.write(html_all)

rfm



customer_metrics

"""Method	Accuracy	Useful For	Correct?

Frequency √ó MonetaryValue	‚≠ê Basic	RFM Segmentation	‚úÖ Yes (basic)

avg_monthly_revenue √ó lifespan_months	‚≠ê‚≠ê Moderate	Historical / Behavioral CLV	‚úÖ Yes (historical)

Both are correct, but for different purposes:

Use RFM CLV for segmentation and initial clustering.

Use Historical CLV for customer valuation and cohort analysis.
"""